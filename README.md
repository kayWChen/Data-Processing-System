# A Data Processing System for LLM-generating Answer Validation


## **Overview**

I have designed and implemented a data processing system. The primary objective of this system is to post-processes the answer text generated by a large language model. It attempts to recognize all named entities from the text, link them to a large knowledge base (WikiData), and ultimately validating the correctness of the generated answer.

![](.\pic\AO.png)

## Major Steps

**Step 1: Input Processing**

- Questions are read from a text file in the organized format.

  ```
   <ID question><TAB>text of the question/completion<newline>
   # for example
   question-001	Is Managua the capital of Nicaragua?
   question-002	Is London the capital of the United Kingdom?
  ```

**Step 2: Question Classification and Answer Generation**

- Utilize a `spaCy`-based question classifier to categorize questions into simple (Yes/No) and special (entity-based) types.
- Generate answer text via LLM.

**Step 3: Entity Recognition and Linking**

- Use `spaCy` for named entity recognition.
- Use `sparqlWrapper` to query mentions in  `Wikidata` to identify candidate entities.
- Adopt a distance-based approach for entity disambiguation.

**Step 4: Relation Extraction**

- A model pre-trained with `Wikidata` dataset is introduced through transformers.
- For special questions, extract relation triplets  from answer text.
- For simple questions, directly extract triplets from the question text.

**Step 5: Fact Checking**

- Query `Wikidata` for correctness verification of relation triplets. We query for the predicate with the two entities in the triplets.
- Check if the predicate and their parent predicates include a  match to the original triplet predicate.
- For special questions, a mismatch implies as “incorrect” for the answer of LLM.
- For simple questions, a mismatch implies as “no” for the answer to the question, but the correctness of LLM answer need to be double checked. We check this by analyzing whether there is a negation in the answer sentence through `spaCy` pattern rules.
- Validate correctness.

**Step 6: Output**

 ```
# Output format as following:
question-001	R"surely you know that Managua is the capital of Nicaragua.
Is Managua a safe place to travel in? it is a very safe city, since if your journey starts with problems and inconveniences you will never leave because this is an adventure you do not want to finish.What is the currency used in Managua? The official currency of Nicaragua is the cordoba."
question-001	A"Yes"
question-001	C"correct"
question-001	E"Managua"	"https://en.wikipedia.org/wiki/Managua"
question-001	E"Nicaragua"	"https://en.wikipedia.org/wiki/Nicaragua"
 ```



## Usage

**Prerequisites**

```shell
pip install -r requirements.txt
```

As `spaCy` library is imported to support with the text processing, the model also need to be installed before running.

```shell
python -m spacy download en
```

**Run the system**

```shell
python wdps.py input_path output_path
```

To run the system, you will need to state your `input_path` and `output_path` in the command line, otherwise the system will search `input.txt` and `output.txt` files from the default path: `./`.

***The first time of running usually takes longer time as models have to be downloaded to local.***



## Scalability Enhancement

The following code is used to parallelize the program to enhance scalability.

```python
import multiprocessing
from multiprocessing import Pool

# Set the process number
cpu_num = multiprocessing.cpu_count()
print(f"{cpu_num} cpus in computation")
pool = Pool(processes=cpu_num) 
# Parallelize the program, each question will be handled as a task sent to a processor
results = pool.map(run_task, [(question_id, question_text) for question_id, question_text in input.items()])  
```

## Performance Evaluation

We created a text file with 10 simple questions and 10 special questions to evaluate the performance of
the system. Specifically, we adopt: number of gold, number of predicted, number of correct, precision,
recall and F1 score as the major performance metrics for the overall system, and specific tasks: answer
extraction, correctness validation, entity recognition. Please see the results in table below. The experiment data can be found in  the `test` file: `input.txt` for the input questions, `output.txt` for the results generated by the system, and `gold_output.txt` for the gold file.

![](.\pic\performance.png)

![](.\pic\perfromance-1.png)

```shell
# How to run the score.py to conduct the experiment
python score.py gold_path output_path
```

